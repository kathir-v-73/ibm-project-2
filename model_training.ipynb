{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87af518",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# TechKey Analysis - Model Training Notebook\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates the machine learning model training process for student risk prediction.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import required libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.svm import SVC\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler, LabelEncoder\\n\",\n",
    "    \"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\\n\",\n",
    "    \"from sklearn.pipeline import Pipeline\\n\",\n",
    "    \"import joblib\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add src to path\\n\",\n",
    "    \"sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd())))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plotting style\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"%matplotlib inline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load and prepare data\\n\",\n",
    "    \"from src.database import get_session\\n\",\n",
    "    \"from src.models import Student, Grade, Attendance\\n\",\n",
    "    \"from src.predictor import prepare_training_data\\n\",\n",
    "    \"\\n\",\n",
    "    \"session = get_session()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Prepare training data using our utility function\\n\",\n",
    "    \"X, y, feature_names = prepare_training_data(session)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training data shape: {X.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Number of features: {len(feature_names)}\\\")\\n\",\n",
    "    \"print(f\\\"Feature names: {feature_names}\\\")\\n\",\n",
    "    \"print(f\\\"Class distribution: {np.bincount(y)}\\\")\\n\",\n",
    "    \"print(f\\\"Percentage of high-risk students: {np.mean(y) * 100:.1f}%\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create DataFrame for EDA\\n\",\n",
    "    \"df_features = pd.DataFrame(X, columns=feature_names)\\n\",\n",
    "    \"df_features['risk_label'] = y\\n\",\n",
    "    \"df_features['risk_level'] = df_features['risk_label'].map({0: 'Low Risk', 1: 'High Risk'})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature distributions by risk level\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 3, figsize=(15, 10))\\n\",\n",
    "    \"axes = axes.ravel()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, feature in enumerate(feature_names):\\n\",\n",
    "    \"    for risk_level in ['Low Risk', 'High Risk']:\\n\",\n",
    "    \"        data = df_features[df_features['risk_level'] == risk_level][feature]\\n\",\n",
    "    \"        axes[i].hist(data, alpha=0.7, label=risk_level, bins=15)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    axes[i].set_title(feature.replace('_', ' ').title())\\n\",\n",
    "    \"    axes[i].set_xlabel('Value')\\n\",\n",
    "    \"    axes[i].set_ylabel('Frequency')\\n\",\n",
    "    \"    axes[i].legend()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Remove empty subplots\\n\",\n",
    "    \"for i in range(len(feature_names), len(axes)):\\n\",\n",
    "    \"    fig.delaxes(axes[i])\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Correlation analysis\\n\",\n",
    "    \"plt.figure(figsize=(10, 8))\\n\",\n",
    "    \"correlation_matrix = df_features[feature_names].corr()\\n\",\n",
    "    \"sns.heatmap(correlation_matrix, \\n\",\n",
    "    \"            annot=True, \\n\",\n",
    "    \"            cmap='coolwarm', \\n\",\n",
    "    \"            center=0,\\n\",\n",
    "    \"            square=True,\\n\",\n",
    "    \"            fmt='.2f')\\n\",\n",
    "    \"plt.title('Feature Correlation Matrix')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Correlation with target\\n\",\n",
    "    \"target_correlations = {}\\n\",\n",
    "    \"for feature in feature_names:\\n\",\n",
    "    \"    correlation = np.corrcoef(df_features[feature], y)[0, 1]\\n\",\n",
    "    \"    target_correlations[feature] = correlation\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Correlation with target variable:\\\")\\n\",\n",
    "    \"for feature, corr in sorted(target_correlations.items(), key=lambda x: abs(x[1]), reverse=True):\\n\",\n",
    "    \"    print(f\\\"{feature:20} : {corr:6.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Split data into training and testing sets\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "    \"    X, y, \\n\",\n",
    "    \"    test_size=0.2, \\n\",\n",
    "    \"    random_state=42, \\n\",\n",
    "    \"    stratify=y\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training set size: {X_train.shape[0]}\\\")\\n\",\n",
    "    \"print(f\\\"Test set size: {X_test.shape[0]}\\\")\\n\",\n",
    "    \"print(f\\\"Training class distribution: {np.bincount(y_train)}\\\")\\n\",\n",
    "    \"print(f\\\"Test class distribution: {np.bincount(y_test)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Define models to evaluate\\n\",\n",
    "    \"models = {\\n\",\n",
    "    \"    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\\n\",\n",
    "    \"    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced'),\\n\",\n",
    "    \"    'SVM': SVC(probability=True, random_state=42, class_weight='balanced')\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate models using cross-validation\\n\",\n",
    "    \"results = {}\\n\",\n",
    "    \"for name, model in models.items():\\n\",\n",
    "    \"    # Create pipeline with scaling for models that need it\\n\",\n",
    "    \"    if name in ['Logistic Regression', 'SVM']:\\n\",\n",
    "    \"        pipeline = Pipeline([\\n\",\n",
    "    \"            ('scaler', StandardScaler()),\\n\",\n",
    "    \"            ('classifier', model)\\n\",\n",
    "    \"        ])\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        pipeline = Pipeline([\\n\",\n",
    "    \"            ('classifier', model)\\n\",\n",
    "    \"        ])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Cross-validation\\n\",\n",
    "    \"    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Fit model\\n\",\n",
    "    \"    pipeline.fit(X_train, y_train)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Predictions\\n\",\n",
    "    \"    y_pred = pipeline.predict(X_test)\\n\",\n",
    "    \"    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Store results\\n\",\n",
    "    \"    results[name] = {\\n\",\n",
    "    \"        'model': pipeline,\\n\",\n",
    "    \"        'cv_mean': cv_scores.mean(),\\n\",\n",
    "    \"        'cv_std': cv_scores.std(),\\n\",\n",
    "    \"        'test_accuracy': (y_pred == y_test).mean(),\\n\",\n",
    "    \"        'roc_auc': roc_auc_score(y_test, y_pred_proba),\\n\",\n",
    "    \"        'y_pred': y_pred,\\n\",\n",
    "    \"        'y_pred_proba': y_pred_proba\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"{name:20} | CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\\\")\\n\",\n",
    "    \"    print(f\\\"{name:20} | Test Accuracy: {results[name]['test_accuracy']:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"{name:20} | ROC AUC: {results[name]['roc_auc']:.3f}\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 50)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compare model performance\\n\",\n",
    "    \"comparison_df = pd.DataFrame({\\n\",\n",
    "    \"    'Model': list(results.keys()),\\n\",\n",
    "    \"    'CV Accuracy': [results[name]['cv_mean'] for name in results],\\n\",\n",
    "    \"    'Test Accuracy': [results[name]['test_accuracy'] for name in results],\\n\",\n",
    "    \"    'ROC AUC': [results[name]['roc_auc'] for name in results]\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== Model Comparison ===\\\")\\n\",\n",
    "    \"print(comparison_df.round(3))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visual comparison\\n\",\n",
    "    \"fig, axes = plt.subplots(1, 2, figsize=(15, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Accuracy comparison\\n\",\n",
    "    \"x_pos = np.arange(len(results))\\n\",\n",
    "    \"width = 0.35\\n\",\n",
    "    \"\\n\",\n",
    "    \"axes[0].bar(x_pos - width/2, comparison_df['CV Accuracy'], width, label='CV Accuracy', alpha=0.7)\\n\",\n",
    "    \"axes[0].bar(x_pos + width/2, comparison_df['Test Accuracy'], width, label='Test Accuracy', alpha=0.7)\\n\",\n",
    "    \"axes[0].set_xlabel('Model')\\n\",\n",
    "    \"axes[0].set_ylabel('Accuracy')\\n\",\n",
    "    \"axes[0].set_title('Model Accuracy Comparison')\\n\",\n",
    "    \"axes[0].set_xticks(x_pos)\\n\",\n",
    "    \"axes[0].set_xticklabels(comparison_df['Model'])\\n\",\n",
    "    \"axes[0].legend()\\n\",\n",
    "    \"\\n\",\n",
    "    # ROC AUC comparison\\n\",\n",
    "    \"axes[1].bar(comparison_df['Model'], comparison_df['ROC AUC'], color='orange', alpha=0.7)\\n\",\n",
    "    \"axes[1].set_xlabel('Model')\\n\",\n",
    "    \"axes[1].set_ylabel('ROC AUC Score')\\n\",\n",
    "    \"axes[1].set_title('Model ROC AUC Comparison')\\n\",\n",
    "    \"axes[1].set_ylim(0, 1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# ROC curves for all models\\n\",\n",
    "    \"plt.figure(figsize=(10, 8))\\n\",\n",
    "    \"\\n\",\n",
    "    \"for name, result in results.items():\\n\",\n",
    "    \"    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\\n\",\n",
    "    \"    plt.plot(fpr, tpr, label=f'{name} (AUC = {result[\\\"roc_auc\\\"]:.3f})', linewidth=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\\n\",\n",
    "    \"plt.xlabel('False Positive Rate')\\n\",\n",
    "    \"plt.ylabel('True Positive Rate')\\n\",\n",
    "    \"plt.title('ROC Curves - Model Comparison')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Select best model\\n\",\n",
    "    \"best_model_name = max(results.keys(), key=lambda x: results[x]['roc_auc'])\\n\",\n",
    "    \"best_model = results[best_model_name]['model']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Best model: {best_model_name}\\\")\\n\",\n",
    "    \"print(f\\\"ROC AUC: {results[best_model_name]['roc_auc']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Test Accuracy: {results[best_model_name]['test_accuracy']:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Detailed classification report for best model\\n\",\n",
    "    \"y_pred_best = results[best_model_name]['y_pred']\\n\",\n",
    "    \"print(\\\"\\\\n=== Classification Report ===\\\")\\n\",\n",
    "    \"print(classification_report(y_test, y_pred_best, target_names=['Low Risk', 'High Risk']))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Confusion matrix\\n\",\n",
    "    \"cm = confusion_matrix(y_test, y_pred_best)\\n\",\n",
    "    \"plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \\n\",\n",
    "    \"            xticklabels=['Low Risk', 'High Risk'],\\n\",\n",
    "    \"            yticklabels=['Low Risk', 'High Risk'])\\n\",\n",
    "    \"plt.title(f'Confusion Matrix - {best_model_name}')\\n\",\n",
    "    \"plt.xlabel('Predicted')\\n\",\n",
    "    \"plt.ylabel('Actual')\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Feature importance for tree-based models\\n\",\n",
    "    \"if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\\n\",\n",
    "    \"    importances = best_model.named_steps['classifier'].feature_importances_\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create feature importance DataFrame\\n\",\n",
    "    \"    feature_importance_df = pd.DataFrame({\\n\",\n",
    "    \"        'feature': feature_names,\\n\",\n",
    "    \"        'importance': importances\\n\",\n",
    "    \"    }).sort_values('importance', ascending=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot feature importance\\n\",\n",
    "    \"    plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"    sns.barplot(data=feature_importance_df, x='importance', y='feature', palette='viridis')\\n\",\n",
    "    \"    plt.title(f'Feature Importance - {best_model_name}')\\n\",\n",
    "    \"    plt.xlabel('Importance')\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"=== Feature Importance ===\\\")\\n\",\n",
    "    \"    print(feature_importance_df.round(4))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Hyperparameter tuning for the best model\\n\",\n",
    "    \"if best_model_name == 'Random Forest':\\n\",\n",
    "    \"    param_grid = {\\n\",\n",
    "    \"        'classifier__n_estimators': [50, 100, 200],\\n\",\n",
    "    \"        'classifier__max_depth': [5, 10, 15, None],\\n\",\n",
    "    \"        'classifier__min_samples_split': [2, 5, 10],\\n\",\n",
    "    \"        'classifier__min_samples_leaf': [1, 2, 4]\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"elif best_model_name == 'Logistic Regression':\\n\",\n",
    "    \"    param_grid = {\\n\",\n",
    "    \"        'classifier__C': [0.1, 1, 10, 100],\\n\",\n",
    "    \"        'classifier__penalty': ['l1', 'l2'],\\n\",\n",
    "    \"        'classifier__solver': ['liblinear']\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"elif best_model_name == 'SVM':\\n\",\n",
    "    \"    param_grid = {\\n\",\n",
    "    \"        'classifier__C': [0.1, 1, 10, 100],\\n\",\n",
    "    \"        'classifier__kernel': ['linear', 'rbf']\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Perform grid search\\n\",\n",
    "    \"print(f\\\"Performing hyperparameter tuning for {best_model_name}...\\\")\\n\",\n",
    "    \"grid_search = GridSearchCV(\\n\",\n",
    "    \"    best_model, \\n\",\n",
    "    \"    param_grid, \\n\",\n",
    "    \"    cv=5, \\n\",\n",
    "    \"    scoring='roc_auc',\\n\",\n",
    "    \"    n_jobs=-1,\\n\",\n",
    "    \"    verbose=1\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"grid_search.fit(X_train, y_train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Best parameters: {grid_search.best_params_}\\\")\\n\",\n",
    "    \"print(f\\\"Best cross-validation score: {grid_search.best_score_:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Update best model with tuned parameters\\n\",\n",
    "    \"best_model_tuned = grid_search.best_estimator_\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Final evaluation on test set\\n\",\n",
    "    \"y_pred_tuned = best_model_tuned.predict(X_test)\\n\",\n",
    "    \"y_pred_proba_tuned = best_model_tuned.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"\\n\",\n",
    "    \"final_accuracy = (y_pred_tuned == y_test).mean()\\n\",\n",
    "    \"final_roc_auc = roc_auc_score(y_test, y_pred_proba_tuned)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== Final Model Performance ===\\\")\\n\",\n",
    "    \"print(f\\\"Accuracy: {final_accuracy:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"ROC AUC: {final_roc_auc:.3f}\\\")\\n\",\n",
    "    \"print(\\\"\\\\n=== Classification Report ===\\\")\\n\",\n",
    "    \"print(classification_report(y_test, y_pred_tuned, target_names=['Low Risk', 'High Risk']))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Compare with baseline\\n\",\n",
    "    \"baseline_accuracy = max(np.mean(y_test), 1 - np.mean(y_test))\\n\",\n",
    "    \"print(f\\\"\\\\nBaseline accuracy (majority class): {baseline_accuracy:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Improvement over baseline: {final_accuracy - baseline_accuracy:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save the trained model\\n\",\n",
    "    \"model_filename = f'../data/trained_model_{best_model_name.lower().replace(\\\" \\\", \\\"_\\\")}.pkl'\\n\",\n",
    "    \"joblib.dump(best_model_tuned, model_filename)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save feature names and model info\\n\",\n",
    "    \"model_info = {\\n\",\n",
    "    \"    'feature_names': feature_names,\\n\",\n",
    "    \"    'model_name': best_model_name,\\n\",\n",
    "    \"    'accuracy': final_accuracy,\\n\",\n",
    "    \"    'roc_auc': final_roc_auc,\\n\",\n",
    "    \"    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\\n\",\n",
    "    \"    'feature_importance': feature_importance_df.to_dict('records') if 'feature_importance_df' in locals() else None\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"joblib.dump(model_info, f'../data/model_info.pkl')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Model saved to: {model_filename}\\\")\\n\",\n",
    "    \"print(f\\\"Model info saved to: ../data/model_info.pkl\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Close database session\\n\",\n",
    "    \"session.close()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Model Training Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Best Model: [Model Name]\\n\",\n",
    "    \"- **ROC AUC**: [Value]\\n\",\n",
    "    \"- **Accuracy**: [Value]\\n\",\n",
    "    \"- **Improvement over baseline**: [Value]\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Key Features (by importance):\\n\",\n",
    "    \"1. [Most important feature]\\n\",\n",
    "    \"2. [Second most important feature]\\n\",\n",
    "    \"3. [Third most important feature]\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Model Insights:\\n\",\n",
    "    \"- The model shows good discrimination between high-risk and low-risk students\\n\",\n",
    "    \"- [Feature] is the strongest predictor of student risk\\n\",\n",
    "    \"- The model achieves [percentage]% accuracy on the test set\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Deployment Recommendations:\\n\",\n",
    "    \"1. Use probability threshold of [value] for high-risk classification\\n\",\n",
    "    \"2. Monitor model performance monthly\\n\",\n",
    "    \"3. Retrain model with new data every [time period]\\n\",\n",
    "    \"4. Focus interventions on students with prediction probability > [threshold]\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
